---
title: "Satellite Telemetry Dataset: adult bearded seals in Kotzebue, Alaska (raw archive)"
subtitle: "Get Data from the Wildlife Computers Data Portal"
draft: true
author:
- name: Josh M. London
  affiliation: 1
- name: Michael F. Cameron
  affiliation: 2
- name: Peter L. Boveng
  affiliation: 2
address:
- code: 1
  address: Alaska Fisheries Science Center, NOAA Fisheries, Seattle, Washington, USA 
  email: josh.london@noaa.gov
  orcid: http://orcid.org/0000-0002-3647-5046
- code: 2
  address: Alaska Fisheries Science Center, NOAA Fisheries, Seattle, Washington, USA 
disclaimer: >
  The scientific results and conclusions, as well as any views or opinions 
  expressed herein, are those of the author(s) and do not necessarily reflect 
  those of NOAA or the Department of Commerce.
abstract: >
  This document describes the process for downloading *raw* telemetry data from
  the Wildlife Computers (https://wildlifecomputers.com/) Data Portal via their
  API. The data are provided as zipped archives for each deployment and are
  preserved as is. The archives are uploaded to the Arctic Data Center
  repository along with relevant metadata and provenance information. Fourteen
  tags were deployed on 7 adult bearded seals that were captured and released
  near Kotzebue, Alaska, USA.
output: 
  uswebr::html_uswds:
    number_sections: FALSE
params:
  get_data: FALSE
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

# Connect to the Data Portal

Wildlife Computers (Redmond, Washington, USA) provides an API for their data
portal that allows us to download the latest telemetry data for any deployment. 
To facilitate connection and use of this API within R, the `wcUtils` package
has been developed.

The data portal is not a public repository. Data access is controlled by the
data owner. Thus, only those users who are either owners of the deployment 
data or users who have been granted access by the owner can access the API.

The `wcUtils` package is available for install from GitHub:

```{r install-wcutils}
if (!require('devtools')) install.packages('devtools')
if (!require('wcUtils')) {
  devtools::install_github('jmlondon/wcUtils')
}
if (!require('tidyverse')) install.packages('tidyverse')
```


# Download KotzEB09 Telemetry

All of the deployments have been labeled within the data portal as 
*ProjectID = KotzEB09*. We will use that information to only download the 
relevant data.

```{r get-data-from-wc, eval = as.logical(params$get_data)}
# first thing, get deployment data from WCDP
r <- wcUtils::wcPOST()
# get KotzEB09 ids and download the data
aleut_ids <- wcUtils::wcGetProjectIDs(r,project = 'KotzEB09')

for (i in 1:length(aleut_ids)) {
  zipfile <- wcUtils::wcGetZip(id = aleut_ids[i])
  file.copy(zipfile,'.',overwrite = TRUE)
  file.rename(file.path(basename(zipfile)),
              file.path(paste(aleut_ids[i],"zip",sep = '.'))
  )
}
```

The zip files downloaded from the data portal are named based on the 
unique id assigned within the data portal. For our deployments, we have 
assigned each a unique `DeployID`. It would be more informative if we could
rename all of these zip files to the assigned `DeployID`. Eventually, this
functionality could be implemented within the `wcUtils` package and the API,
but for now, we'll just open each zip file and examine the data files to
extract the assigned `DeployID`.

```{r rename-zip-files, eval = as.logical(params$get_data)}
df <- tibble::as_tibble()
for (zipfile in list.files(pattern = ".zip",full.names = TRUE)) {
  summary_name <- grep('*-Summary.csv',
                      unzip(file.path(
                                      basename(zipfile)),
                            list = TRUE)$Name,
                       value = TRUE)
  deployid <- read.csv(unzip(zipfile, files = summary_name))$DeployID
  deployid <- as.character(deployid)
  instr <- read.csv(unzip(zipfile, files = summary_name))$Instr
  instr <- as.character(instr)
  first_tx <- read.csv(unzip(zipfile, files = summary_name))$LatestXmitTime
  first_tx <- as.character(first_tx)
  last_tx <- read.csv(unzip(zipfile, files = summary_name))$LatestXmitTime
  last_tx <- as.character(last_tx)
  ndays_tx <- read.csv(unzip(zipfile, files = summary_name))$XmitDays
  ndays_tx <- as.character(ndays_tx)
  
  file.remove(summary_name)
  file.rename(file.path(zipfile),
              file.path(paste(deployid,"zip",sep = '.'))
  )
  
  df <- df %>% dplyr::bind_rows(list(filename = paste(deployid,"zip",sep = '.'),
                 instrument = instr,
                 "first transmission" = first_tx,
                 "last transmission" = last_tx,
                 "no. days" = ndays_tx
                 ))
}
```

## List of Archive Files

```{r list-downloaded-files}
df %>% knitr::kable()
```

# Create datapack

Now that we have our files downloaded and stored locally, we want to create a
datapack for upload to DataONE. The process described here borrows heavily from
the [vignette provided DataONE](https://cran.r-project.org/web/packages/dataone/vignettes/upload-data.html) in the `dataone` package.

First, we load the required libraries and then create an empty DataPackage.

```{r create-datapack}
if(!require('dataone')) install.packages('dataone')
if(!require('datapack')) install.packages('datapack')
if(!require('uuid')) install.packages('uuid')

dp <- new("DataPackage")
```

```{r add-files-datapack}
metadataObj <- new("DataObject", 
                   format = "https://www.iso.org/standard/53798.html",
                   filename = "kotzeb0912_raw_iso19115.xml")
dp <- addMember(dp, metadataObj)

for (zipfile in list.files(pattern = ".zip")) {
 sourceObj <- new("DataObject",format = "application/zip",filename = zipfile)
 dp <- addMember(dp, sourceObj, metadataObj)
}

progObj <- new("DataObject", format = "application/R", 
               filename = "00_kotzeb0912_get_data.Rmd", 
               mediaType = "text/x-rsrc")
dp <- addMember(dp, progObj, mo = metadataObj)
```

# Establish Connection with DataONE Node

# Send datapack to DataONE Node
